#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MISA CRM Data Loader
T√≠ch h·ª£p v·ªõi TikTok Shop Infrastructure - C·∫•u tr√∫c src/
"""

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging
import sys
import os

# Import shared utilities
sys.path.append(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)
from config.settings import settings
from src.utils.logging import setup_logging

logger = setup_logging(__name__)


class MISACRMLoader:
    """
    MISA CRM Data Loader - T∆∞∆°ng t·ª± TikTok Shop Loader pattern
    """

    def __init__(self):
        """Kh·ªüi t·∫°o MISA CRM Loader"""
        self.db_engine = create_engine(settings.sql_server_connection_string)

        # Table mapping - ph·∫£i kh·ªõp v·ªõi keys t·ª´ transformer
        self.table_mappings = {
            "customers": settings.get_misa_crm_table_full_name("misa_customers"),
            "sale_orders_flattened": settings.get_misa_crm_table_full_name(
                "misa_sale_orders_flattened"
            ),
            "contacts": settings.get_misa_crm_table_full_name("misa_contacts"),
            "stocks": settings.get_misa_crm_table_full_name("misa_stocks"),
            "products": settings.get_misa_crm_table_full_name("misa_products"),
        }

        logger.info(f"Kh·ªüi t·∫°o MISA CRM Loader cho {settings.company_name}")
        logger.info(f"Database: {settings.sql_server_host}")

    def _get_table_info(self, table_full_name: str) -> Dict[str, Any]:
        """
        L·∫•y th√¥ng tin v·ªÅ table (schema, table name)

        Args:
            table_full_name: T√™n ƒë·∫ßy ƒë·ªß c·ªßa table (schema.table)

        Returns:
            Dict ch·ª©a schema v√† table name
        """
        parts = table_full_name.split(".")
        if len(parts) == 2:
            return {"schema": parts[0], "table": parts[1]}
        else:
            return {"schema": "staging", "table": table_full_name}

    def truncate_table(self, endpoint: str) -> bool:
        """
        Truncate staging table cho endpoint

        Args:
            endpoint: T√™n endpoint

        Returns:
            True n·∫øu th√†nh c√¥ng
        """
        if endpoint not in self.table_mappings:
            logger.error(f"Kh√¥ng t√¨m th·∫•y table mapping cho endpoint: {endpoint}")
            return False

        table_full_name = self.table_mappings[endpoint]

        try:
            # FIXED: S·ª≠ d·ª•ng pyodbc connection thay v√¨ SQLAlchemy ƒë·ªÉ tr√°nh l·ªói commit
            import pyodbc

            with pyodbc.connect(settings.pyodbc_connection_string) as conn:
                cursor = conn.cursor()
                cursor.execute(f"TRUNCATE TABLE {table_full_name}")
                conn.commit()

            logger.info(f"Truncated table {table_full_name}")
            return True

        except Exception as e:
            logger.error(f"L·ªói khi truncate table {table_full_name}: {e}")
            return False

    def load_dataframe_to_staging(
        self, df: pd.DataFrame, endpoint: str, if_exists: str = "append"
    ) -> bool:
        """
        Load DataFrame v√†o staging table

        Args:
            df: DataFrame c·∫ßn load
            endpoint: T√™n endpoint
            if_exists: H√†nh ƒë·ªông n·∫øu table ƒë√£ t·ªìn t·∫°i ('append', 'replace', 'fail')

        Returns:
            True n·∫øu th√†nh c√¥ng
        """
        if df.empty:
            logger.warning(f"DataFrame r·ªóng cho endpoint {endpoint}")
            return True

        if endpoint not in self.table_mappings:
            logger.error(f"Kh√¥ng t√¨m th·∫•y table mapping cho endpoint: {endpoint}")
            return False

        table_full_name = self.table_mappings[endpoint]
        table_info = self._get_table_info(table_full_name)

        try:
            # Chu·∫©n h√≥a DataFrame theo schema v√† endpoint tr∆∞·ªõc khi ghi
            df = self._normalize_dataframe_for_endpoint(df, endpoint, table_info)

            # X·ª≠ l√Ω tr√πng kh√≥a cho sale_orders_flattened tr∆∞·ªõc khi ghi ƒë·ªÉ tr√°nh l·ªói PK
            if endpoint == "sale_orders_flattened":
                # B·ªè c√°c d√≤ng thi·∫øu kh√≥a
                before_len = len(df)
                df = df.dropna(
                    subset=["order_id", "item_id"]
                )  # item_product_code c√≥ th·ªÉ NULL theo schema m·ªõi
                dropped_nulls = before_len - len(df)
                if dropped_nulls > 0:
                    logger.info(
                        f"sale_orders_flattened: b·ªè {dropped_nulls} d√≤ng thi·∫øu kh√≥a (order_id/item_id) tr∆∞·ªõc khi load"
                    )

                # Lo·∫°i to√†n b·ªô b·∫£n ghi tr√πng tr√™n b·ªô kh√≥a (order_id, item_id)
                dup_mask = df.duplicated(subset=["order_id", "item_id"], keep=False)
                dup_count = int(dup_mask.sum())
                if dup_count > 0:
                    logger.warning(
                        f"sale_orders_flattened: ph√°t hi·ªán {dup_count} d√≤ng tr√πng kh√≥a trong batch ‚Äî s·∫Ω lo·∫°i b·ªè to√†n b·ªô c√°c d√≤ng tr√πng ƒë·ªÉ tr√°nh l·ªói PK"
                    )
                    df = df[~dup_mask]

                # X√≥a tr∆∞·ªõc c√°c b·∫£n ghi tr√πng trong DB ƒë·ªÉ tr√°nh xung ƒë·ªôt khi insert
                try:
                    self._predelete_conflicting_sale_order_items(df, table_info)
                except Exception as predel_err:
                    logger.warning(
                        f"sale_orders_flattened: kh√¥ng th·ªÉ pre-delete kh√≥a tr√πng trong DB ({predel_err}), ti·∫øp t·ª•c ghi d·ªØ li·ªáu c√≤n l·∫°i"
                    )

            # FIXED: B·ªè method="multi" ƒë·ªÉ tr√°nh l·ªói parameter markers v·ªõi SQL Server
            batch_size = min(
                50, settings.misa_crm_etl_batch_size
            )  # Gi·∫£m batch size xu·ªëng 50

            # Load data using pandas to_sql v·ªõi batch size nh·ªè h∆°n
            df.to_sql(
                name=table_info["table"],
                con=self.db_engine,
                schema=table_info["schema"],
                if_exists=if_exists,
                index=False,
                # method="multi",  # FIXED: B·ªè method="multi" v√¨ kh√¥ng t∆∞∆°ng th√≠ch v·ªõi SQL Server
                chunksize=batch_size,  # FIXED: Batch size nh·ªè h∆°n
            )

            logger.info(f"Loaded {len(df)} records to {table_full_name}")
            return True

        except Exception as e:
            logger.error(f"L·ªói khi load data v√†o {table_full_name}: {e}")
            # Try alternative loading method for all tables (SQLAlchemy engine issue)
            logger.info(f"Trying alternative pyodbc loading method for {endpoint}...")
            return self._load_with_pyodbc(df, table_full_name)

    def load_incremental_data(self, endpoint: str, df: pd.DataFrame) -> bool:
        """
        Load data incrementally using UPSERT (INSERT/UPDATE) logic
        Similar to TikTok Shop loader pattern

        Args:
            endpoint: MISA CRM endpoint name (customers, sale_orders, etc.)
            df: DataFrame with data to load

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            if df.empty:
                logger.warning(f"DataFrame is empty for {endpoint}, nothing to load")
                return True

            logger.info(
                f"Loading {len(df)} rows incrementally for {endpoint} with UPSERT logic..."
            )

            # Prepare the data
            df_prepared = self._prepare_dataframe_for_upsert(df, endpoint)
            if df_prepared is None:
                return False

            # Use MERGE statement for proper UPSERT
            return self._upsert_records(endpoint, df_prepared)

        except Exception as e:
            logger.error(f"Error in incremental load for {endpoint}: {str(e)}")
            return False

    def _prepare_dataframe_for_upsert(
        self, df: pd.DataFrame, endpoint: str
    ) -> Optional[pd.DataFrame]:
        """
        Prepare DataFrame for UPSERT operation

        Args:
            df: Original DataFrame
            endpoint: MISA CRM endpoint name

        Returns:
            Prepared DataFrame or None if error
        """
        try:
            df_prepared = df.copy()

            # Add ETL metadata columns
            current_time = datetime.now()
            df_prepared["etl_batch_id"] = (
                f"misa_crm_{endpoint}_{current_time.strftime('%Y%m%d_%H%M%S')}"
            )
            df_prepared["etl_created_at"] = current_time
            df_prepared["etl_updated_at"] = current_time

            # Handle NaN values
            # Gi·ªØ None ƒë·ªÉ ghi NULL l√™n DB thay v√¨ chu·ªói r·ªóng

            return df_prepared

        except Exception as e:
            logger.error(f"Error preparing DataFrame for {endpoint}: {str(e)}")
            return None

    def _upsert_records(self, endpoint: str, df: pd.DataFrame) -> bool:
        """
        Perform UPSERT operation using SQL MERGE statement
        Similar to TikTok Shop pattern but adapted for MISA CRM endpoints

        Args:
            endpoint: MISA CRM endpoint name
            df: Prepared DataFrame

        Returns:
            bool: True if successful, False otherwise
        """
        if endpoint not in self.table_mappings:
            logger.error(f"No table mapping found for endpoint: {endpoint}")
            return False

        table_full_name = self.table_mappings[endpoint]
        table_info = self._get_table_info(table_full_name)

        try:
            with self.db_engine.connect() as conn:
                # Create temporary table
                temp_table = (
                    f"#temp_{endpoint}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                )

                # Load data to temp table first (gi·∫£m chunksize ƒë·ªÉ tr√°nh l·ªói ODBC parameter markers)
                df.to_sql(
                    name=temp_table.replace("#", ""),
                    con=conn,
                    if_exists="replace",
                    index=False,
                    method="multi",
                    chunksize=15,
                )

                # Get primary key for each endpoint
                primary_key = self._get_primary_key_for_endpoint(endpoint)

                # Perform MERGE operation
                merge_sql = self._build_merge_sql(
                    endpoint, table_info, temp_table, primary_key, df.columns.tolist()
                )

                result = conn.execute(text(merge_sql))
                rows_affected = result.rowcount

                # Drop temp table
                conn.execute(text(f"DROP TABLE {temp_table}"))

                logger.info(
                    f"UPSERT completed for {endpoint}: {rows_affected} rows affected"
                )
                return True

        except Exception as e:
            logger.error(f"Error in _upsert_records for {endpoint}: {str(e)}")
            return False

    def _get_primary_key_for_endpoint(self, endpoint: str) -> str:
        """
        Get primary key column name for each MISA CRM endpoint

        Args:
            endpoint: MISA CRM endpoint name

        Returns:
            Primary key column name
        """
        primary_keys = {
            "customers": "customer_id",
            "sale_orders_flattened": "order_id",
            "contacts": "contact_id",
            "stocks": "stock_id",
            "products": "product_id",
        }

        return primary_keys.get(endpoint, "id")  # Default fallback

    def _get_update_condition_for_endpoint(self, endpoint: str) -> Optional[str]:
        """
        Get the update condition for the MERGE statement for a given endpoint.
        This is used to only update rows that have actually changed.
        Compares source and target columns based on `ModifiedOn` timestamp.

        Args:
            endpoint: MISA CRM endpoint name

        Returns:
            Update condition string or None
        """
        # Using ModifiedOn as the primary indicator of change
        conditions = {
            "customers": "target.ModifiedOn < source.ModifiedOn",
            "sale_orders_flattened": "target.ModifiedOn < source.ModifiedOn",
            "contacts": "target.ModifiedOn < source.ModifiedOn",
            "stocks": "target.ModifiedOn < source.ModifiedOn",
            "products": "target.ModifiedOn < source.ModifiedOn",
        }
        return conditions.get(endpoint)

    def _build_merge_sql(
        self,
        endpoint: str,
        table_info: Dict,
        temp_table: str,
        primary_key: str,
        columns: List[str],
        update_condition: Optional[str] = None,
    ) -> str:
        """
        Build SQL MERGE statement for UPSERT operation

        Args:
            endpoint: MISA CRM endpoint name
            table_info: Table schema and name info
            temp_table: Temporary table name
            primary_key: Primary key column name
            columns: List of DataFrame columns

        Returns:
            SQL MERGE statement
        """
        schema = table_info["schema"]
        table = table_info["table"]

        # Filter out ETL metadata columns for matching conditions
        data_columns = [col for col in columns if not col.startswith("etl_")]

        # Build UPDATE SET clause
        update_set = []
        for col in data_columns:
            if col != primary_key:  # Don't update primary key
                update_set.append(f"target.{col} = source.{col}")

        # Add ETL metadata update
        update_set.append("target.etl_updated_at = GETDATE()")

        # Build INSERT columns and values
        insert_columns = ", ".join(columns)
        insert_values = ", ".join([f"source.{col}" for col in columns])

        merge_sql = f"""
        MERGE [{schema}].[{table}] AS target
        USING {temp_table} AS source
        ON target.{primary_key} = source.{primary_key}

        WHEN MATCHED THEN
            UPDATE SET
                {', '.join(update_set)}

        WHEN NOT MATCHED THEN
            INSERT ({insert_columns})
            VALUES ({insert_values});
        """

        return merge_sql

    def _load_with_pyodbc(self, df: pd.DataFrame, table_full_name: str) -> bool:
        """
        Alternative loading method using pyodbc for composite key tables
        """
        try:
            import pyodbc

            # Create connection string for pyodbc
            connection_string = (
                f"DRIVER={{ODBC Driver 18 for SQL Server}};"
                f"SERVER={settings.sql_server_host},{settings.sql_server_port};"
                f"DATABASE={settings.sql_server_database};"
                f"UID={settings.sql_server_username};"
                f"PWD={settings.sql_server_password};"
                f"TrustServerCertificate=yes"
            )

            connection = pyodbc.connect(connection_string)
            cursor = connection.cursor()

            # Get table info
            table_info = self._get_table_info(table_full_name)
            schema = table_info["schema"]
            table = table_info["table"]

            # Get table columns (excluding computed columns)
            cursor.execute(
                f"""
                SELECT COLUMN_NAME
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = '{schema}'
                AND TABLE_NAME = '{table}'
                AND COLUMNPROPERTY(OBJECT_ID(TABLE_SCHEMA+'.'+TABLE_NAME), COLUMN_NAME, 'IsComputed') = 0
                ORDER BY ORDINAL_POSITION
            """
            )

            db_columns = [row.COLUMN_NAME for row in cursor.fetchall()]

            # Match DataFrame columns with database columns
            matching_columns = [col for col in db_columns if col in df.columns]

            # DEBUG: Log column mismatch details
            missing_in_df = [col for col in db_columns if col not in df.columns]
            extra_in_df = [col for col in df.columns if col not in db_columns]

            if missing_in_df:
                logger.warning(f"Columns missing in DataFrame: {missing_in_df}")
            if extra_in_df:
                logger.warning(f"Extra columns in DataFrame: {extra_in_df}")

            logger.info(
                f"Matched {len(matching_columns)} columns out of {len(db_columns)} database columns"
            )
            # FIXED: S·∫Øp x·∫øp DataFrame columns theo th·ª© t·ª± database ƒë·ªÉ tr√°nh l·ªói column order

            if matching_columns:
                df_ordered = df[matching_columns]
                logger.info(f"Reordered DataFrame columns to match database order")
            else:
                df_ordered = df

            if not matching_columns:
                logger.error(
                    f"No matching columns found between DataFrame and {table_full_name}"
                )
                logger.error(f"DataFrame columns: {sorted(df.columns.tolist())}")
                logger.error(f"Database columns: {sorted(db_columns)}")
                return False

            # Prepare insert statement
            placeholders = ", ".join(["?" for _ in matching_columns])
            insert_sql = f"INSERT INTO {schema}.{table} ({', '.join(matching_columns)}) VALUES ({placeholders})"

            # Insert data in batches - FIXED: Gi·∫£m batch size ƒë·ªÉ tr√°nh parameter limit
            batch_size = min(
                100, len(df)
            )  # Gi·∫£m batch size xu·ªëng 100 ƒë·ªÉ tr√°nh parameter limit
            total_inserted = 0

            for i in range(0, len(df), batch_size):
                batch_df = df_ordered.iloc[i : i + batch_size]
                batch_data = []

                for _, row in batch_df.iterrows():
                    row_data = []
                    for col in matching_columns:
                        value = row[col] if col in row else None
                        # FIXED: Handle NaN values v√† data type conversion
                        if pd.isna(value):
                            row_data.append(None)
                        else:
                            # Convert data types ƒë·ªÉ tr√°nh type mismatch
                            converted_value = self._convert_value_for_sql(value)
                            row_data.append(converted_value)
                    batch_data.append(row_data)

                # Execute batch insert
                cursor.executemany(insert_sql, batch_data)
                connection.commit()
                total_inserted += len(batch_data)

                logger.info(
                    f"   Inserted batch {i//batch_size + 1}: {len(batch_data)} rows"
                )

            cursor.close()
            connection.close()

            logger.info(
                f"Successfully loaded {total_inserted} records to {table_full_name} using pyodbc"
            )
            return True

        except Exception as e:
            logger.error(f"pyodbc loading failed for {table_full_name}: {e}")
            return False

    def _convert_value_for_sql(self, value):
        """
        Convert value t·ªëi gi·∫£n ƒë·ªÉ t∆∞∆°ng th√≠ch SQL Server data types.
        Sau khi ƒë√£ normalize DataFrame, h√†m n√†y ch·ªâ x·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ph·ªï bi·∫øn.
        """
        if value is None or pd.isna(value):
            return None

        # Datetime
        if isinstance(value, (pd.Timestamp, datetime)):
            if hasattr(value, "tz") and value.tz is not None:
                value = (
                    value.tz_convert(None) if hasattr(value, "tz_convert") else value
                )
            try:
                return value.strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                return str(value)

        # Integer/Float/Bool/Str gi·ªØ nguy√™n d·∫°ng ph√π h·ª£p
        if isinstance(value, (int, np.integer)):
            return int(value)

        if isinstance(value, (float, np.floating)):
            return float(value)

        if isinstance(value, bool):
            return value

        if isinstance(value, str):
            return value

        # M·∫∑c ƒë·ªãnh: stringify
        return str(value)

    def _normalize_datetime_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Chu·∫©n h√≥a c√°c c·ªôt datetime: parse ISO/epoch, b·ªè timezone, thay NaT b·∫±ng None.
        √Åp d·ª•ng an to√†n cho m·ªçi DataFrame (kh√¥ng thay ƒë·ªïi c·ªôt kh√¥ng ph·∫£i datetime).
        """
        df_norm = df.copy()
        for col in df_norm.columns:
            s = df_norm[col]
            # B·ªè qua n·∫øu l√† c√°c ki·ªÉu kh√¥ng li√™n quan
            if pd.api.types.is_datetime64_any_dtype(s):
                # Lo·∫°i b·ªè timezone (n·∫øu c√≥) v√† thay NaT b·∫±ng None
                try:
                    s2 = s.dt.tz_convert(None)
                except Exception:
                    s2 = s
                df_norm[col] = s2.where(pd.notna(s2), None)
            else:
                # N·∫øu l√† numeric c√≥ th·ªÉ l√† epoch (ms/s) ho·∫∑c string ISO
                if pd.api.types.is_numeric_dtype(s):
                    s_float = s.astype("float64")
                    if s_float.dropna().gt(1e12).any():
                        dt = pd.to_datetime(
                            s_float, unit="ms", errors="coerce", utc=True
                        )
                    elif s_float.dropna().gt(1e9).any():
                        dt = pd.to_datetime(
                            s_float, unit="s", errors="coerce", utc=True
                        )
                    else:
                        continue
                    dt = dt.dt.tz_convert(None)
                    df_norm[col] = dt.where(pd.notna(dt), None)
                elif pd.api.types.is_string_dtype(s):
                    dt = pd.to_datetime(s, errors="coerce", utc=True)
                    if dt.notna().any():
                        dt = dt.dt.tz_convert(None)
                        df_norm[col] = dt.where(pd.notna(dt), None)
        return df_norm

    def _normalize_dataframe_for_endpoint(
        self, df: pd.DataFrame, endpoint: str, table_info: Dict[str, Any]
    ) -> pd.DataFrame:
        """
        - L·ªçc/ƒëi·ªÅu ch·ªânh c·ªôt theo schema ƒë√≠ch
        - Chu·∫©n h√≥a d·ªØ li·ªáu THEO KI·ªÇU C·ªòT trong schema:
          + DATETIME/DATETIME2: parse ISO/epoch ‚Üí tz-naive, NaT‚ÜíNone
          + DECIMAL/NUMERIC/INT: √©p s·ªë `to_numeric(errors='coerce')`
          + BIT: map v·ªÅ True/False ho·∫∑c None
          + NVARCHAR: c√≥ th·ªÉ thay "" ‚Üí None ƒë·ªÉ s·∫°ch
        """
        df_norm = df.copy()

        # L·∫•y danh s√°ch c·ªôt & ki·ªÉu d·ªØ li·ªáu t·ª´ DB
        try:
            with self.db_engine.connect() as conn:
                cols = conn.execute(
                    text(
                        """
                        SELECT COLUMN_NAME, DATA_TYPE
                        FROM INFORMATION_SCHEMA.COLUMNS
                        WHERE TABLE_SCHEMA = :schema AND TABLE_NAME = :table
                        ORDER BY ORDINAL_POSITION
                        """
                    ),
                    {"schema": table_info["schema"], "table": table_info["table"]},
                ).fetchall()
                db_columns = [r[0] for r in cols]
                column_types = {r[0]: str(r[1]).lower() for r in cols}
        except Exception:
            db_columns = df_norm.columns.tolist()
            column_types = {c: "nvarchar" for c in db_columns}

        # N·∫øu endpoint l√† customers v√† schema c√≥ account_code nh∆∞ng ngu·ªìn kh√¥ng c√≥ ‚Üí b·ªè c·ªôt n√†y
        if (
            endpoint == "customers"
            and "account_code" in db_columns
            and "account_code" not in df_norm.columns
        ):
            db_columns = [c for c in db_columns if c != "account_code"]

        # Gi·ªØ l·∫°i c·ªôt c√≥ trong DB
        keep_cols = [c for c in db_columns if c in df_norm.columns]
        if keep_cols:
            df_norm = df_norm[keep_cols]

        # Chu·∫©n h√≥a theo ki·ªÉu c·ªôt trong schema
        df_norm = self._normalize_by_schema_types(df_norm, column_types)

        return df_norm

    def _predelete_conflicting_sale_order_items(
        self, df: pd.DataFrame, table_info: Dict[str, Any]
    ) -> None:
        """
        X√≥a tr∆∞·ªõc c√°c b·∫£n ghi trong DB c√≥ kh√≥a (order_id, item_id) tr√πng v·ªõi batch hi·ªán t·∫°i
        ƒë·ªÉ tr√°nh l·ªói tr√πng kh√≥a khi insert. H√†m an to√†n: n·∫øu batch r·ªóng s·∫Ω b·ªè qua.
        """
        if df.empty:
            return

        # L·∫•y c·∫∑p kh√≥a duy nh·∫•t trong batch
        keys_df = df[["order_id", "item_id"]].dropna().drop_duplicates()
        if keys_df.empty:
            return

        schema = table_info["schema"]
        table = table_info["table"]

        # T·∫°o danh s√°ch gi√° tr·ªã cho c√¢u l·ªánh DELETE IN (VALUES ...)
        tuples = list(keys_df.itertuples(index=False, name=None))
        values_clause = ", ".join([f"({int(oid)}, {int(iid)})" for oid, iid in tuples])

        sql = f"""
        DELETE tgt
        FROM [{schema}].[{table}] AS tgt
        JOIN (VALUES {values_clause}) AS src(order_id, item_id)
          ON tgt.order_id = src.order_id AND tgt.item_id = src.item_id
        """

        with self.db_engine.connect() as conn:
            conn.execute(text(sql))
            conn.commit()

    def _normalize_by_schema_types(
        self, df: pd.DataFrame, column_types: Dict[str, str]
    ) -> pd.DataFrame:
        df_norm = df.copy()

        datetime_types = {"datetime", "datetime2", "smalldatetime", "date"}
        numeric_types = {
            "decimal",
            "numeric",
            "float",
            "real",
            "int",
            "bigint",
            "smallint",
            "tinyint",
        }

        for col in df_norm.columns:
            dtype = column_types.get(col, "nvarchar")
            s = df_norm[col]

            # DATETIME
            if dtype in datetime_types:
                if pd.api.types.is_datetime64_any_dtype(s):
                    try:
                        s2 = s.dt.tz_convert(None)
                    except Exception:
                        s2 = s
                    df_norm[col] = s2.where(pd.notna(s2), None)
                else:
                    if pd.api.types.is_numeric_dtype(s):
                        s_float = s.astype("float64")
                        if s_float.dropna().gt(1e12).any():
                            dt = pd.to_datetime(
                                s_float, unit="ms", errors="coerce", utc=True
                            )
                        elif s_float.dropna().gt(1e9).any():
                            dt = pd.to_datetime(
                                s_float, unit="s", errors="coerce", utc=True
                            )
                        else:
                            dt = pd.to_datetime(s_float, errors="coerce", utc=True)
                    else:
                        dt = pd.to_datetime(s, errors="coerce", utc=True)
                    dt = dt.dt.tz_convert(None)
                    df_norm[col] = dt.where(pd.notna(dt), None)
                continue

            # NUMERIC
            if dtype in numeric_types:
                if pd.api.types.is_string_dtype(s):
                    s_clean = (
                        s.str.replace("%", "", regex=False)
                        .str.replace(",", "", regex=False)
                        .str.strip()
                    )
                else:
                    s_clean = s
                df_norm[col] = pd.to_numeric(s_clean, errors="coerce")
                continue

            # BIT
            if dtype == "bit":
                if pd.api.types.is_bool_dtype(s):
                    df_norm[col] = s
                else:

                    def _to_bool(v):
                        if pd.isna(v):
                            return None
                        if isinstance(v, bool):
                            return v
                        vs = str(v).strip().lower()
                        if vs in {"1", "true", "yes", "on"}:
                            return True
                        if vs in {"0", "false", "no", "off"}:
                            return False
                        return None

                    df_norm[col] = s.map(_to_bool)
                continue

            # NVARCHAR: thay chu·ªói r·ªóng th√†nh NULL ƒë·ªÉ s·∫°ch d·ªØ li·ªáu
            if pd.api.types.is_string_dtype(s):
                df_norm[col] = s.replace("", None)

        return df_norm

    def load_all_data_to_staging(
        self, transformed_data: Dict[str, pd.DataFrame], truncate_first: bool = False
    ) -> Dict[str, int]:
        """
        Load t·∫•t c·∫£ transformed data v√†o staging tables

        Args:
            transformed_data: Dict v·ªõi key l√† endpoint name, value l√† DataFrame
            truncate_first: C√≥ truncate tables tr∆∞·ªõc khi load kh√¥ng

        Returns:
            Dict v·ªõi s·ªë records ƒë√£ load cho m·ªói endpoint
        """
        logger.info("B·∫Øt ƒë·∫ßu load t·∫•t c·∫£ data v√†o staging tables...")

        loaded_counts = {}

        for endpoint, df in transformed_data.items():
            if df.empty:
                logger.warning(f"DataFrame r·ªóng cho {endpoint}, b·ªè qua")
                loaded_counts[endpoint] = 0
                continue

            try:
                # Truncate table n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
                if truncate_first:
                    self.truncate_table(endpoint)

                # Load data
                success = self.load_dataframe_to_staging(
                    df, endpoint, if_exists="append"
                )

                if success:
                    loaded_counts[endpoint] = len(df)
                    logger.info(f"{endpoint}: {len(df)} records loaded")
                else:
                    loaded_counts[endpoint] = 0
                    logger.error(f"{endpoint}: Load th·∫•t b·∫°i")

            except Exception as e:
                logger.error(f"Exception khi load {endpoint}: {e}")
                loaded_counts[endpoint] = 0

        total_loaded = sum(loaded_counts.values())
        failed_endpoints = [
            endpoint
            for endpoint, count in loaded_counts.items()
            if count == 0 and not transformed_data[endpoint].empty
        ]

        if failed_endpoints:
            error_msg = f"Load th·∫•t b·∫°i cho c√°c endpoints: {failed_endpoints}"
            logger.error(error_msg)
            raise RuntimeError(error_msg)

        logger.info(f"Load ho√†n th√†nh: {total_loaded} t·ªïng records")
        return loaded_counts

    def validate_loaded_data(self, loaded_counts: Dict[str, int]) -> Dict[str, Any]:
        """
        Validate d·ªØ li·ªáu ƒë√£ load v√†o staging tables

        Args:
            loaded_counts: Dict v·ªõi s·ªë records ƒë√£ load

        Returns:
            Dict v·ªõi validation results
        """
        logger.info("ƒêang validate d·ªØ li·ªáu ƒë√£ load...")

        validation_results = {
            "total_expected_records": sum(loaded_counts.values()),
            "total_actual_records": 0,
            "table_validations": {},
            "validation_passed": True,
        }

        for endpoint, expected_count in loaded_counts.items():
            if endpoint not in self.table_mappings:
                continue

            table_full_name = self.table_mappings[endpoint]

            try:
                with self.db_engine.connect() as conn:
                    # Count records in table
                    result = conn.execute(
                        text(f"SELECT COUNT(*) FROM {table_full_name}")
                    )
                    actual_count = result.fetchone()[0]

                    # Check latest ETL batch
                    result = conn.execute(
                        text(f"SELECT MAX(etl_created_at) FROM {table_full_name}")
                    )
                    latest_etl_time = result.fetchone()[0]

                    table_validation = {
                        "expected_count": expected_count,
                        "actual_count": actual_count,
                        "count_match": actual_count
                        >= expected_count,  # Allow for existing data
                        "latest_etl_time": latest_etl_time,
                        "has_recent_data": latest_etl_time
                        and (datetime.now() - latest_etl_time).total_seconds()
                        < 3600,  # Within 1 hour
                    }

                    validation_results["table_validations"][endpoint] = table_validation
                    validation_results["total_actual_records"] += actual_count

                    if (
                        not table_validation["count_match"]
                        or not table_validation["has_recent_data"]
                    ):
                        validation_results["validation_passed"] = False

                    logger.info(
                        f"üìä {endpoint}: Expected {expected_count}, Actual {actual_count}, Latest ETL: {latest_etl_time}"
                    )

            except Exception as e:
                logger.error(f"L·ªói khi validate {endpoint}: {e}")
                validation_results["validation_passed"] = False
                validation_results["table_validations"][endpoint] = {"error": str(e)}

        logger.info(
            f"Validation t·ªïng th·ªÉ: {'PASSED' if validation_results['validation_passed'] else 'FAILED'}"
        )

        return validation_results

    def get_staging_data_summary(self) -> Dict[str, Any]:
        """
        L·∫•y t√≥m t·∫Øt d·ªØ li·ªáu trong staging tables

        Returns:
            Dict v·ªõi th√¥ng tin t√≥m t·∫Øt
        """
        logger.info("ƒêang l·∫•y t√≥m t·∫Øt d·ªØ li·ªáu staging...")

        summary = {
            "timestamp": datetime.now().isoformat(),
            "tables": {},
            "total_records": 0,
        }

        for endpoint, table_full_name in self.table_mappings.items():
            try:
                with self.db_engine.connect() as conn:
                    # Basic counts
                    result = conn.execute(
                        text(f"SELECT COUNT(*) FROM {table_full_name}")
                    )
                    total_count = result.fetchone()[0]

                    # Latest ETL info
                    result = conn.execute(
                        text(
                            f"""
                        SELECT
                            MAX(etl_created_at) as latest_etl,
                            COUNT(DISTINCT etl_batch_id) as batch_count
                        FROM {table_full_name}
                    """
                        )
                    )
                    etl_info = result.fetchone()

                    # Recent data (last 24 hours)
                    result = conn.execute(
                        text(
                            f"""
                        SELECT COUNT(*)
                        FROM {table_full_name}
                        WHERE etl_created_at >= DATEADD(day, -1, GETDATE())
                    """
                        )
                    )
                    recent_count = result.fetchone()[0]

                    table_summary = {
                        "total_records": total_count,
                        "recent_records_24h": recent_count,
                        "latest_etl_time": etl_info[0],
                        "total_batches": etl_info[1],
                    }

                    summary["tables"][endpoint] = table_summary
                    summary["total_records"] += total_count

                    logger.info(
                        f"üìä {endpoint}: {total_count} records, {recent_count} recent"
                    )

            except Exception as e:
                logger.error(f"L·ªói khi l·∫•y summary cho {endpoint}: {e}")
                summary["tables"][endpoint] = {"error": str(e)}

        logger.info(f"üìä T·ªïng records trong staging: {summary['total_records']}")

        return summary

    def cleanup_old_data(self, retention_days: int = None) -> Dict[str, int]:
        """
        Cleanup d·ªØ li·ªáu c≈© trong staging tables

        Args:
            retention_days: S·ªë ng√†y gi·ªØ l·∫°i d·ªØ li·ªáu (None = s·ª≠ d·ª•ng config)

        Returns:
            Dict v·ªõi s·ªë records ƒë√£ x√≥a
        """
        if retention_days is None:
            retention_days = settings.misa_crm_data_retention_days

        logger.info(f"ƒêang cleanup d·ªØ li·ªáu c≈© h∆°n {retention_days} ng√†y...")

        deleted_counts = {}

        for endpoint, table_full_name in self.table_mappings.items():
            try:
                with self.db_engine.connect() as conn:
                    # Delete old data
                    result = conn.execute(
                        text(
                            f"""
                        DELETE FROM {table_full_name}
                        WHERE etl_created_at < DATEADD(day, -{retention_days}, GETDATE())
                    """
                        )
                    )

                    deleted_count = result.rowcount
                    deleted_counts[endpoint] = deleted_count

                    conn.commit()

                    if deleted_count > 0:
                        logger.info(f"üóëÔ∏è {endpoint}: ƒê√£ x√≥a {deleted_count} records c≈©")
                    else:
                        logger.info(f"{endpoint}: Kh√¥ng c√≥ d·ªØ li·ªáu c≈© c·∫ßn x√≥a")

            except Exception as e:
                logger.error(f"L·ªói khi cleanup {endpoint}: {e}")
                deleted_counts[endpoint] = 0

        total_deleted = sum(deleted_counts.values())
        logger.info(f"üóëÔ∏è Cleanup ho√†n th√†nh: {total_deleted} t·ªïng records ƒë√£ x√≥a")

        return deleted_counts

    def test_database_connection(self) -> bool:
        """
        Test database connection

        Returns:
            True n·∫øu connection th√†nh c√¥ng
        """
        try:
            with self.db_engine.connect() as conn:
                result = conn.execute(text("SELECT 1"))
                test_value = result.fetchone()[0]

                if test_value == 1:
                    logger.info("Database connection test th√†nh c√¥ng")
                    return True
                else:
                    logger.error("Database connection test th·∫•t b·∫°i")
                    return False

        except Exception as e:
            logger.error(f"Database connection error: {e}")
            return False
