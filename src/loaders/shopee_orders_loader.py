#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Shopee Orders Data Loader
T√≠ch h·ª£p v·ªõi Facolos Enterprise ETL Infrastructure
"""

import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging
import sys
import os

# Import shared utilities
sys.path.append(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
)
from config.settings import settings
from src.utils.logging import setup_logging

logger = setup_logging(__name__)


class ShopeeOrderLoader:
    """
    Shopee Orders Data Loader - T∆∞∆°ng t·ª± TikTok Shop v√† MISA CRM pattern
    """

    def __init__(self):
        """Kh·ªüi t·∫°o Shopee Order Loader"""
        self.db_engine = create_engine(settings.sql_server_connection_string)

        # Table mappings - s·ª≠ d·ª•ng staging schema, b·∫£ng Shopee c√≥ ti·ªÅn t·ªë shopee_
        self.table_mappings = {
            "orders": settings.get_table_full_name("shopee", "shopee_orders"),
            "recipient_address": settings.get_table_full_name(
                "shopee", "shopee_recipient_address"
            ),
            "order_items": settings.get_table_full_name("shopee", "shopee_order_items"),
            "order_item_locations": settings.get_table_full_name(
                "shopee", "shopee_order_item_locations"
            ),
            "packages": settings.get_table_full_name("shopee", "shopee_packages"),
            "package_items": settings.get_table_full_name(
                "shopee", "shopee_package_items"
            ),
            "invoice": settings.get_table_full_name("shopee", "shopee_invoice"),
            "payment_info": settings.get_table_full_name(
                "shopee", "shopee_payment_info"
            ),
            "order_pending_terms": settings.get_table_full_name(
                "shopee", "shopee_order_pending_terms"
            ),
            "order_warnings": settings.get_table_full_name(
                "shopee", "shopee_order_warnings"
            ),
            "prescription_images": settings.get_table_full_name(
                "shopee", "shopee_prescription_images"
            ),
            "buyer_proof_of_collection": settings.get_table_full_name(
                "shopee", "shopee_buyer_proof_of_collection"
            ),
        }

        logger.info(f"Kh·ªüi t·∫°o Shopee Order Loader cho {settings.company_name}")
        logger.info(f"Database: {settings.sql_server_host}")
        logger.info(f"Schema: {settings.schema_mappings.get('shopee', 'staging')}")

    def _get_table_info(self, table_full_name: str) -> Dict[str, Any]:
        """
        L·∫•y th√¥ng tin v·ªÅ table (schema, table name)

        Args:
            table_full_name: T√™n ƒë·∫ßy ƒë·ªß c·ªßa table (schema.table)

        Returns:
            Dict ch·ª©a schema v√† table name
        """
        parts = table_full_name.split(".")
        if len(parts) == 2:
            return {"schema": parts[0], "table": parts[1]}
        else:
            return {"schema": "staging", "table": table_full_name}

    def _convert_datetime_to_naive(self, df: pd.DataFrame) -> pd.DataFrame:
        """Chuy·ªÉn datetime tz-aware v·ªÅ tz-naive theo m√∫i gi·ªù Vi·ªát Nam (+07)."""
        df_copy = df.copy()
        for col in df_copy.columns:
            if df_copy[col].dtype == "datetime64[ns, UTC]":
                df_copy[col] = (
                    df_copy[col].dt.tz_convert("Asia/Ho_Chi_Minh").dt.tz_localize(None)
                )
            elif "datetime" in str(df_copy[col].dtype):
                df_copy[col] = (
                    pd.to_datetime(df_copy[col], utc=True)
                    .dt.tz_convert("Asia/Ho_Chi_Minh")
                    .dt.tz_localize(None)
                )
        return df_copy

    def _normalize_datetime_fields(self, df: pd.DataFrame) -> pd.DataFrame:
        """Chu·∫©n h√≥a c√°c c·ªôt th·ªùi gian t·ª´ UNIX epoch (s/ms) ho·∫∑c ISO string v·ªÅ datetime tz-naive.

        Ch·ªâ √°p d·ª•ng cho c√°c c·ªôt th·ªùi gian c·ªßa Shopee orders theo schema staging.shopee_orders.
        """
        if df.empty:
            return df

        df_norm = df.copy()

        datetime_cols = [
            "create_time",
            "update_time",
            "ship_by_date",
            "note_update_time",
            "pay_time",
            "pickup_done_time",
            "edt_from",
            "edt_to",
            "return_request_due_date",
            "prescription_approval_time",
            "prescription_rejection_time",
        ]

        for col in datetime_cols:
            if col not in df_norm.columns:
                continue

            s = df_norm[col]

            # B·ªè qua ho√†n to√†n n·∫øu series ƒë√£ l√† datetime64[ns]
            if str(s.dtype).startswith("datetime64"):
                continue

            # Tr∆∞·ªùng h·ª£p numeric (epoch gi√¢y/ms)
            if pd.api.types.is_numeric_dtype(s):
                s_float = s.astype("float64")
                # Heuristic: >1e12 => ms, >1e9 => s
                if s_float.dropna().gt(1e12).any():
                    dt = pd.to_datetime(s_float, unit="ms", errors="coerce", utc=True)
                elif s_float.dropna().gt(1e9).any():
                    dt = pd.to_datetime(s_float, unit="s", errors="coerce", utc=True)
                else:
                    dt = pd.to_datetime(s_float, unit="s", errors="coerce", utc=True)
                df_norm[col] = (
                    dt.dt.tz_convert("Asia/Ho_Chi_Minh").dt.tz_localize(None)
                    if dt.dt.tz is not None
                    else dt
                )
                continue

            # Tr∆∞·ªùng h·ª£p string ISO ho·∫∑c chu·ªói kh√°c
            if pd.api.types.is_string_dtype(s):
                # Th·ª≠ parse ISO; pandas h·ªó tr·ª£ h·∫≠u t·ªë Z n·∫øu utc=True
                dt = pd.to_datetime(s, errors="coerce", utc=True)
                if dt.notna().any():
                    df_norm[col] = (
                        dt.dt.tz_convert("Asia/Ho_Chi_Minh").dt.tz_localize(None)
                        if dt.dt.tz is not None
                        else dt
                    )

        return df_norm

    def truncate_table(self, table_name: str) -> bool:
        """
        X√≥a t·∫•t c·∫£ d·ªØ li·ªáu trong table (Full Load) theo chu·∫©n SQL Server

        Args:
            table_name: T√™n table c·∫ßn truncate

        Returns:
            True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        table_full_name = self.table_mappings.get(table_name)
        if not table_full_name:
            logger.error(f"‚ùå Table mapping not found for: {table_name}")
            return False

        try:
            with self.db_engine.begin() as conn:
                schema, table = table_full_name.split(".")

                # V·ªõi SQL Server, TRUNCATE TABLE kh√¥ng th·ªÉ d√πng khi c√≥ FK tham chi·∫øu.
                # V√¨ b·∫£ng orders ƒë∆∞·ª£c tham chi·∫øu b·ªüi nhi·ªÅu b·∫£ng con, thay th·∫ø b·∫±ng DELETE theo th·ª© t·ª± an to√†n.
                if table_name == "orders":
                    logger.info(
                        "‚Ü©Ô∏è Detected parent table 'orders' ‚Äî performing safe cascade DELETE on child tables first"
                    )
                    self._delete_shopee_children_tables(conn, schema)

                # Th·ª±c hi·ªán DELETE thay cho TRUNCATE ƒë·ªÉ kh√¥ng v∆∞·ªõng FK
                delete_sql = f"DELETE FROM {table_full_name}"
                result = conn.execute(text(delete_sql))

                logger.info(
                    f"‚úÖ Cleared table via DELETE: {table_full_name} (rows affected: {result.rowcount})"
                )
                return True

        except Exception as e:
            logger.error(f"‚ùå Failed to truncate table {table_full_name}: {str(e)}")
            return False

    def _delete_shopee_children_tables(self, conn, schema: str) -> None:
        """X√≥a d·ªØ li·ªáu c√°c b·∫£ng con c·ªßa Shopee theo ƒë√∫ng th·ª© t·ª± ƒë·ªÉ ƒë·∫£m b·∫£o r√†ng bu·ªôc FK.

        Th·ª© t·ª± x√≥a (child -> parent):
          - package_items (tham chi·∫øu packages, order_items)
          - order_item_locations (tham chi·∫øu order_items)
          - packages (tham chi·∫øu orders)
          - invoice (tham chi·∫øu orders)
          - payment_info (tham chi·∫øu orders)
          - order_pending_terms (tham chi·∫øu orders)
          - order_warnings (tham chi·∫øu orders)
          - prescription_images (tham chi·∫øu orders)
          - buyer_proof_of_collection (tham chi·∫øu orders)
          - order_items (tham chi·∫øu orders)
          - recipient_address (tham chi·∫øu orders)
        """
        tables_in_order = [
            "shopee_package_items",
            "shopee_order_item_locations",
            "shopee_packages",
            "shopee_invoice",
            "shopee_payment_info",
            "shopee_order_pending_terms",
            "shopee_order_warnings",
            "shopee_prescription_images",
            "shopee_buyer_proof_of_collection",
            "shopee_order_items",
            "shopee_recipient_address",
        ]

        for tbl in tables_in_order:
            full_name = f"{schema}.{tbl}"
            try:
                res = conn.execute(
                    text(
                        f"IF OBJECT_ID('{full_name}', 'U') IS NOT NULL DELETE FROM {full_name}"
                    )
                )
                # M·ªôt s·ªë driver kh√¥ng tr·∫£ rowcount cho c√¢u IF... n√™n c·∫ßn try/except ri√™ng
                affected = getattr(res, "rowcount", None)
                logger.info(
                    f"   üóëÔ∏è Cleared child table: {full_name}{'' if affected is None else f' (rows: {affected})'}"
                )
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Skipped deleting {full_name}: {e}")

    def load_dataframe_to_table(
        self, df: pd.DataFrame, table_name: str, if_exists: str = "append"
    ) -> bool:
        """
        Load DataFrame v√†o table

        Args:
            df: DataFrame c·∫ßn load
            table_name: T√™n table ƒë√≠ch
            if_exists: X·ª≠ l√Ω khi table ƒë√£ t·ªìn t·∫°i ('append', 'replace', 'fail')

        Returns:
            True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        if df.empty:
            logger.warning(f"‚ö†Ô∏è DataFrame for {table_name} is empty, skipping")
            return True

        table_full_name = self.table_mappings.get(table_name)
        if not table_full_name:
            logger.error(f"‚ùå Table mapping not found for: {table_name}")
            return False

        try:
            # X√≥a duplicate cho t·∫•t c·∫£ b·∫£ng Shopee (full load)
            df_deduped = self._deduplicate_shopee_dataframe(df, table_name)

            if df_deduped.empty:
                logger.warning(
                    f"‚ö†Ô∏è DataFrame for {table_name} is empty after deduplication, skipping"
                )
                return True

            # Convert datetime columns to timezone-naive
            df_export = self._convert_datetime_to_naive(df_deduped)

            # B·ªï sung etl_* theo +07 n·∫øu thi·∫øu (full load d√πng INSERT qua to_sql)
            current_time_vn = pd.Timestamp.now(tz="Asia/Ho_Chi_Minh").tz_localize(None)
            if "etl_created_at" not in df_export.columns:
                df_export["etl_created_at"] = current_time_vn
            if "etl_updated_at" not in df_export.columns:
                df_export["etl_updated_at"] = current_time_vn

            # Load to database (gi·ªõi h·∫°n chunksize nh·ªè ƒë·ªÉ tr√°nh qu√° t·∫£i tham s·ªë ODBC/SQL Server)
            df_export.to_sql(
                name=table_full_name.split(".")[1],  # Table name only
                con=self.db_engine,
                schema=table_full_name.split(".")[0],  # Schema name
                if_exists=if_exists,
                index=False,
                method="multi",
                chunksize=15,
            )

            logger.info(f"‚úÖ Loaded {len(df_deduped)} rows to {table_full_name}")
            return True

        except Exception as e:
            logger.error(f"‚ùå Failed to load DataFrame to {table_full_name}: {str(e)}")
            return False

    def _deduplicate_shopee_dataframe(
        self, df: pd.DataFrame, table_name: str
    ) -> pd.DataFrame:
        """
        X√≥a duplicate theo kh√≥a ch√≠nh c·ªßa t·ª´ng b·∫£ng Shopee (ch·ªâ cho full load)
        """
        # Kh√≥a ch√≠nh theo b·∫£ng (theo sql/00_master_setup.sql)
        pk_map = {
            "orders": ["order_sn"],
            "recipient_address": ["order_sn"],
            "order_items": ["order_sn", "order_item_id", "model_id"],
            "order_item_locations": [
                "order_sn",
                "order_item_id",
                "model_id",
                "location_id",
            ],
            "packages": ["order_sn", "package_number"],
            "package_items": [
                "order_sn",
                "package_number",
                "order_item_id",
                "model_id",
            ],
            "invoice": ["order_sn"],
            "payment_info": ["order_sn", "transaction_id"],
            "order_pending_terms": ["order_sn", "term"],
            "order_warnings": ["order_sn", "warning"],
            "prescription_images": ["order_sn", "image_url"],
            "buyer_proof_of_collection": ["order_sn", "image_url"],
        }

        primary_keys = pk_map.get(table_name, [])
        if not primary_keys:
            logger.warning(
                f"‚ö†Ô∏è No primary key mapping for {table_name}, skipping deduplication"
            )
            return df

        # Ki·ªÉm tra c√°c c·ªôt kh√≥a ch√≠nh c√≥ t·ªìn t·∫°i kh√¥ng
        missing_cols = [col for col in primary_keys if col not in df.columns]
        if missing_cols:
            logger.warning(
                f"‚ö†Ô∏è Missing primary key columns {missing_cols} for {table_name}, skipping deduplication"
            )
            return df

        # X√≥a duplicate, gi·ªØ l·∫°i b·∫£n ghi cu·ªëi c√πng
        original_count = len(df)
        df_deduped = df.drop_duplicates(subset=primary_keys, keep="last")
        removed_count = original_count - len(df_deduped)

        if removed_count > 0:
            logger.info(
                f"üîÑ Deduplicated {table_name}: removed {removed_count} duplicates ({original_count} ‚Üí {len(df_deduped)})"
            )

        return df_deduped

    def load_orders_full_load(self, dataframes: Dict[str, pd.DataFrame]) -> bool:
        """
        Load d·ªØ li·ªáu full load cho t·∫•t c·∫£ c√°c b·∫£ng Shopee

        Args:
            dataframes: Dictionary ch·ª©a c√°c DataFrame theo ERD

        Returns:
            True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        logger.info("üöÄ Starting Shopee full load data loading...")

        try:
            # Load theo th·ª© t·ª± ƒë·ªÉ tr√°nh foreign key constraint
            load_order = [
                "orders",  # Main table first
                "recipient_address",
                "order_items",
                "order_item_locations",
                "packages",
                "package_items",
                "invoice",
                "payment_info",
                "order_pending_terms",
                "order_warnings",
                "prescription_images",
                "buyer_proof_of_collection",
            ]

            success_count = 0
            total_count = len(load_order)

            for table_name in load_order:
                if table_name in dataframes:
                    df = dataframes[table_name]

                    if not df.empty:
                        # Truncate table tr∆∞·ªõc khi load (full load)
                        if self.truncate_table(table_name):
                            if self.load_dataframe_to_table(df, table_name, "append"):
                                success_count += 1
                                logger.info(
                                    f"‚úÖ Successfully loaded {table_name}: {len(df)} rows"
                                )
                            else:
                                logger.error(f"‚ùå Failed to load {table_name}")
                        else:
                            logger.error(f"‚ùå Failed to truncate {table_name}")
                    else:
                        logger.info(f"üì≠ Skipping empty {table_name}")
                        success_count += 1  # Empty table is considered success

            if success_count == total_count:
                logger.info(
                    f"üéâ Full load completed successfully: {success_count}/{total_count} tables"
                )
                return True
            else:
                logger.error(
                    f"‚ùå Full load failed: {success_count}/{total_count} tables"
                )
                return False

        except Exception as e:
            logger.error(f"‚ùå Full load failed with exception: {str(e)}")
            return False

    def _clean_dataframe_for_upsert(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        L√†m s·∫°ch DataFrame tr∆∞·ªõc khi UPSERT ƒë·ªÉ tr√°nh l·ªói pyodbc
        """
        if df.empty:
            return df

        df_clean = df.copy()
        fixed_issues = []

        # X·ª≠ l√Ω NaT values cho datetime columns
        datetime_columns = df_clean.select_dtypes(include=["datetime64"]).columns
        for col in datetime_columns:
            na_count = df_clean[col].isna().sum()
            df_clean[col] = df_clean[col].replace({pd.NaT: None})
            if na_count > 0:
                fixed_issues.append(f"Fixed {na_count} NaT values in {col}")

        # X·ª≠ l√Ω NaN values cho t·∫•t c·∫£ columns
        df_clean = df_clean.where(pd.notnull(df_clean), None)

        # X·ª≠ l√Ω string columns c√≥ gi√° tr·ªã 'nan', 'N/A', 'null', 'None'
        for col in df_clean.columns:
            if df_clean[col].dtype == "object":
                df_clean[col] = (
                    df_clean[col]
                    .astype(str)
                    .replace(["nan", "N/A", "null", "NULL", "None", "none", ""], None)
                )

        # X·ª≠ l√Ω numeric columns - convert string numbers to numeric
        numeric_columns = df_clean.select_dtypes(include=["int64", "float64"]).columns
        for col in numeric_columns:
            df_clean[col] = pd.to_numeric(df_clean[col], errors="coerce")

        # Log c√°c v·∫•n ƒë·ªÅ ƒë√£ fix
        if fixed_issues:
            logger.info(f"Fixed data quality issues: {fixed_issues}")

        return df_clean

    def upsert_table(self, df: pd.DataFrame, table_name: str) -> bool:
        """
        UPSERT (MERGE + VALUES) cho t·ª´ng b·∫£ng Shopee theo kh√≥a t·ª± nhi√™n v·ªõi x·ª≠ l√Ω l·ªói pyodbc ƒë∆∞·ª£c c·∫£i thi·ªán.
        - Kh√¥ng t·∫°o b·∫£ng t·∫°m.
        - UPDATE c√≥ guard theo update_time n·∫øu c·ªôt n√†y t·ªìn t·∫°i v√†/ho·∫∑c tr∆∞·ªùng quan tr·ªçng.
        - X·ª≠ l√Ω data quality v√† batch size t·ªëi ∆∞u.
        """
        if df.empty:
            logger.info(f"üì≠ No data to upsert for Shopee.{table_name}")
            return True

        table_full_name = self.table_mappings.get(table_name)
        if not table_full_name:
            logger.error(f"‚ùå Table mapping not found for: {table_name}")
            return False

        # L√†m s·∫°ch DataFrame tr∆∞·ªõc khi x·ª≠ l√Ω
        df_clean = self._clean_dataframe_for_upsert(df)

        # Batch size t·ªëi ∆∞u cho Shopee: 40 rows cho 50 c·ªôt
        # 40√ó50 = 2000 parameters < 2100 limit c·ªßa SQL Server
        if table_name == "orders":
            batch_size = min(40, len(df_clean))
        else:
            batch_size = min(100, len(df_clean))

        logger.info(
            f"UPSERT {len(df_clean)} rows to Shopee.{table_name} with batch_size={batch_size}"
        )

        try:
            return self._execute_upsert_batch(df_clean, table_name, batch_size)
        except Exception as e:
            logger.error(f"Batch UPSERT failed for Shopee.{table_name}: {e}")
            # Fallback: x·ª≠ l√Ω t·ª´ng row m·ªôt
            return self._upsert_records_row_by_row(df_clean, table_name)

    def _execute_upsert_batch(
        self, df: pd.DataFrame, table_name: str, batch_size: int
    ) -> bool:
        """
        Th·ª±c hi·ªán UPSERT batch v·ªõi error handling t·ªët h∆°n cho Shopee Orders
        """
        if df.empty:
            return True

        table_full_name = self.table_mappings.get(table_name)
        if not table_full_name:
            logger.error(f"‚ùå Table mapping not found for: {table_name}")
            return False

        schema, target_table = table_full_name.split(".")

        # Kh√≥a t·ª± nhi√™n theo b·∫£ng
        pk_map = {
            # Theo sql/00_master_setup.sql
            "orders": ["order_sn"],
            "recipient_address": ["order_sn"],
            # PRIMARY KEY (order_sn, order_item_id, model_id)
            "order_items": ["order_sn", "order_item_id", "model_id"],
            # PRIMARY KEY (order_sn, order_item_id, model_id, location_id)
            "order_item_locations": [
                "order_sn",
                "order_item_id",
                "model_id",
                "location_id",
            ],
            # PRIMARY KEY (order_sn, package_number)
            "packages": ["order_sn", "package_number"],
            # PRIMARY KEY (order_sn, package_number, order_item_id, model_id)
            "package_items": [
                "order_sn",
                "package_number",
                "order_item_id",
                "model_id",
            ],
            # PRIMARY KEY (order_sn)
            "invoice": ["order_sn"],
            # PRIMARY KEY (order_sn, transaction_id)
            "payment_info": ["order_sn", "transaction_id"],
            # PRIMARY KEY (order_sn, term)
            "order_pending_terms": ["order_sn", "term"],
            # PRIMARY KEY (order_sn, warning)
            "order_warnings": ["order_sn", "warning"],
            # PRIMARY KEY (order_sn, image_url)
            "prescription_images": ["order_sn", "image_url"],
            # PRIMARY KEY (order_sn, image_url)
            "buyer_proof_of_collection": ["order_sn", "image_url"],
        }

        primary_keys = pk_map.get(table_name)
        if not primary_keys:
            logger.error(f"‚ùå Missing primary key mapping for Shopee.{table_name}")
            return False

        # L·∫•y danh s√°ch c·ªôt th·∫≠t c·ªßa b·∫£ng ƒë√≠ch
        db_columns = []
        try:
            with self.db_engine.connect() as conn:
                rows = conn.execute(
                    text(
                        """
                        SELECT COLUMN_NAME
                        FROM INFORMATION_SCHEMA.COLUMNS
                        WHERE TABLE_SCHEMA = :schema AND TABLE_NAME = :table
                        ORDER BY ORDINAL_POSITION
                        """
                    ),
                    {"schema": schema, "table": target_table},
                ).fetchall()
                db_columns = [r[0] for r in rows]
                if not db_columns:
                    raise RuntimeError("Empty INFORMATION_SCHEMA result")
        except Exception:
            try:
                with self.db_engine.connect() as conn:
                    res = conn.execute(
                        text(f"SELECT TOP 0 * FROM [{schema}].[{target_table}]")
                    )
                    db_columns = list(res.keys())
            except Exception as e2:
                logger.error(
                    f"Kh√¥ng th·ªÉ l·∫•y danh s√°ch c·ªôt DB cho {table_full_name}: {e2}"
                )
                return False

        # ƒê·∫£m b·∫£o c√°c c·ªôt kh√≥a c√≥ trong DataFrame
        for pk in primary_keys:
            if pk not in df.columns:
                logger.error(
                    f"‚ùå Primary key column '{pk}' not found in DataFrame for Shopee.{table_name}"
                )
                return False

        # X√≥a duplicate theo kh√≥a ch√≠nh tr∆∞·ªõc khi upsert
        df_deduped = self._deduplicate_shopee_dataframe(df, table_name)
        if df_deduped.empty:
            logger.info(
                f"üì≠ No data to upsert after deduplication for Shopee.{table_name}"
            )
            return True

        # ƒê·∫£m b·∫£o c√≥ ƒë·ªß c√°c c·ªôt c·∫ßn thi·∫øt cho schema
        if "source_request_id" not in df_deduped.columns:
            df_deduped["source_request_id"] = None
        if "ingested_at" not in df_deduped.columns:
            df_deduped["ingested_at"] = pd.Timestamp.now(
                tz="Asia/Ho_Chi_Minh"
            ).tz_localize(None)

        # Thi·∫øt l·∫≠p etl_* theo +07 cho ƒë∆∞·ªùng MERGE (INSERT)
        current_time_vn = pd.Timestamp.now(tz="Asia/Ho_Chi_Minh").tz_localize(None)
        if "etl_created_at" not in df_deduped.columns:
            df_deduped["etl_created_at"] = current_time_vn
        if "etl_updated_at" not in df_deduped.columns:
            df_deduped["etl_updated_at"] = current_time_vn

        # Chu·∫©n h√≥a datetime: chuy·ªÉn epoch/ISO -> datetime, r·ªìi b·ªè timezone
        df_deduped = self._normalize_datetime_fields(df_deduped)
        df_deduped = self._convert_datetime_to_naive(df_deduped)

        # Danh s√°ch c·ªôt theo DataFrame, intersect v·ªõi DB columns
        df_columns: List[str] = df_deduped.columns.tolist()
        columns: List[str] = [c for c in df_columns if c in db_columns]
        if not columns:
            logger.error(
                f"Kh√¥ng c√≥ c·ªôt n√†o c·ªßa DataFrame kh·ªõp v·ªõi b·∫£ng {table_full_name}. DF cols: {df_columns} ‚Äî DB cols: {db_columns}"
            )
            return False

        # Th√™m c√°c c·ªôt thi·∫øu v√†o DataFrame v·ªõi gi√° tr·ªã NULL
        missing_df_cols = [c for c in db_columns if c not in df_columns]
        if missing_df_cols:
            logger.warning(
                f"Shopee.{table_name}: Th√™m {len(missing_df_cols)} c·ªôt thi·∫øu v·ªõi gi√° tr·ªã NULL"
            )
            for c in missing_df_cols:
                df_deduped[c] = None

        # Lo·∫°i b·ªè c·ªôt kh√¥ng c√≥ trong DB
        extra_df_cols = [c for c in df_columns if c not in db_columns]
        if extra_df_cols:
            logger.info(
                f"Shopee.{table_name}: Lo·∫°i b·ªè c·ªôt kh√¥ng c√≥ trong DB: {extra_df_cols}"
            )
            df_deduped = df_deduped.drop(columns=extra_df_cols, errors="ignore")

        # Reorder DataFrame columns theo th·ª© t·ª± DB
        df_deduped = df_deduped[db_columns]

        # Ch·ªët l·∫°i NULL an to√†n sau khi th√™m c·ªôt thi·∫øu (theo ph√¢n t√≠ch ch√≠nh x√°c)
        # ƒê·∫£m b·∫£o kh√¥ng c√≤n NaN/NA/NaT trong to√†n b·ªô DataFrame
        df_deduped = df_deduped.replace({np.nan: None})
        df_deduped = df_deduped.where(pd.notnull(df_deduped), None)

        # L·∫•y l·∫°i columns sau m·ªçi ch·ªânh s·ª≠a ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng nh·∫•t
        columns = df_deduped.columns.tolist()

        # B·∫£o ƒë·∫£m c√°c c·ªôt kh√≥a c√≥ trong DataFrame
        for pk in primary_keys:
            if pk not in columns:
                logger.error(
                    f"Primary key column '{pk}' not found in DataFrame for Shopee.{table_name}"
                )
                return False

        # X√¢y d·ª±ng c√°c ph·∫ßn t·ª≠ c·ªßa c√¢u MERGE
        col_list_sql = ", ".join([f"[{c}]" for c in columns])
        on_clause = " AND ".join([f"target.{pk} = source.{pk}" for pk in primary_keys])

        # Lo·∫°i b·ªè c√°c c·ªôt ETL kh·ªèi auto-update
        etl_cols = {"etl_created_at", "etl_updated_at", "etl_batch_id", "etl_source"}
        update_set_cols = [
            c for c in columns if c not in primary_keys and c not in etl_cols
        ]
        set_clauses = [f"target.{c} = source.{c}" for c in update_set_cols]

        # Gi·ªØ batch/source n·∫øu c√≥ trong ngu·ªìn
        if "etl_batch_id" in columns:
            set_clauses.append("target.etl_batch_id = source.etl_batch_id")
        if "etl_source" in columns:
            set_clauses.append("target.etl_source = source.etl_source")

        # C·∫≠p nh·∫≠t m·ªëc ETL theo gi·ªù Vi·ªát Nam (+07)
        set_clauses.append("target.etl_updated_at = DATEADD(HOUR, 7, GETUTCDATE())")
        update_set_sql = ",\n                        ".join(set_clauses)
        insert_values_sql = ", ".join([f"source.{c}" for c in columns])

        # Update guard logic
        update_guard = None
        if "update_time" in df_deduped.columns:
            update_guard = "ISNULL(target.update_time, '1900-01-01') < ISNULL(source.update_time, '1900-01-01')"

        extra_changes = []
        if table_name == "orders":
            if "order_status" in df_deduped.columns:
                extra_changes.append(
                    "ISNULL(target.order_status,'') <> ISNULL(source.order_status,'')"
                )
            if "shipping_carrier" in df_deduped.columns:
                extra_changes.append(
                    "ISNULL(target.shipping_carrier,'') <> ISNULL(source.shipping_carrier,'')"
                )

        if extra_changes:
            update_guard = f"({update_guard} OR {' OR '.join(extra_changes)})"

        matched_guard = (
            f"WHEN MATCHED THEN"
            if not update_guard
            else f"WHEN MATCHED AND {update_guard} THEN"
        )

        try:
            with self.db_engine.begin() as conn:
                total_rows = 0
                records = df_deduped.to_dict(orient="records")
                for i in range(0, len(records), batch_size):
                    batch = records[i : i + batch_size]

                    # X√¢y VALUES v√† tham s·ªë r√†ng bu·ªôc
                    values_rows = []
                    params: Dict[str, Any] = {}
                    for r_idx, row in enumerate(batch):
                        placeholders = []
                        for c in columns:
                            pname = f"p_{r_idx}_{c}"
                            placeholders.append(f":{pname}")
                            val = row.get(c, None)
                            # Tuy·ªát ƒë·ªëi d√πng pd.isna, ƒë·ª´ng ch·ªâ check float (theo ph√¢n t√≠ch ch√≠nh x√°c)
                            if pd.isna(val):
                                val = None
                            params[pname] = val
                        values_rows.append(f"({', '.join(placeholders)})")

                    values_sql = ",\n                        ".join(values_rows)

                    merge_sql = f"""
                    MERGE [{schema}].[{target_table}] AS target
                    USING (
                        VALUES
                            {values_sql}
                    ) AS source ({col_list_sql})
                    ON {on_clause}

                    {matched_guard}
                        UPDATE SET
                            {update_set_sql}

                    WHEN NOT MATCHED BY TARGET THEN
                        INSERT ({col_list_sql})
                        VALUES ({insert_values_sql});
                    """

                    conn.execute(text(merge_sql), params)
                    total_rows += len(batch)

            logger.info(
                f"UPSERT (no-temp) completed for Shopee.{table_name}: {len(df_deduped)} rows processed"
            )
            return True

        except Exception as e:
            logger.error(
                f"Error in _execute_upsert_batch for Shopee.{table_name}: {str(e)}"
            )
            raise

    def _upsert_records_row_by_row(self, df: pd.DataFrame, table_name: str) -> bool:
        """
        UPSERT t·ª´ng row m·ªôt khi batch UPSERT th·∫•t b·∫°i cho Shopee Orders
        """
        if df.empty:
            return True

        success_count = 0
        error_count = 0
        total_rows = len(df)

        logger.info(
            f"Starting row-by-row UPSERT for Shopee.{table_name}: {total_rows} rows"
        )

        for index, row in df.iterrows():
            try:
                # T·∫°o DataFrame v·ªõi 1 row
                single_row_df = pd.DataFrame([row])

                # Th·ª±c hi·ªán UPSERT cho 1 row
                result = self._execute_upsert_batch(single_row_df, table_name, 1)
                if result:
                    success_count += 1
                else:
                    error_count += 1

            except Exception as e:
                error_count += 1
                logger.error(f"Row {index} failed for Shopee.{table_name}: {e}")

                # N·∫øu qu√° nhi·ªÅu l·ªói, d·ª´ng l·∫°i
                if error_count > 10:
                    logger.error(
                        f"Too many errors ({error_count}), stopping upsert for Shopee.{table_name}"
                    )
                    break

        # T√≠nh success rate th·ª±c t·∫ø
        actual_success_rate = (success_count / total_rows) * 100
        actual_error_rate = (error_count / total_rows) * 100

        # Log k·∫øt qu·∫£ th·ª±c t·∫ø
        logger.info(
            f"Row-by-row UPSERT completed for Shopee.{table_name}: {success_count}/{total_rows} ({actual_success_rate:.1f}%) success, {error_count}/{total_rows} ({actual_error_rate:.1f}%) errors"
        )

        return success_count > 0

    # C√°c ti·ªán √≠ch ki·ªÉm th·ª≠/ƒë·∫øm b·∫£n ghi kh√¥ng c√≤n d√πng trong pipeline ch√≠nh ƒë√£ ƒë∆∞·ª£c lo·∫°i b·ªè
