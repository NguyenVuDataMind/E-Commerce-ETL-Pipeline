"""
TrÃ­ch xuáº¥t dá»¯ liá»‡u tá»« TikTok Shop API
Dá»±a trÃªn implementation                      # Request body cho POST request - FULL LOAD: Láº¥y Táº¤T Cáº¢ orders
                # Tá»« 1/7/2024 Ä‘áº¿n hiá»‡n táº¡i (theo yÃªu cáº§u business)
                request_body = {
                    'create_time_ge': int(datetime(2024, 7, 1).timestamp()),  # Tá»« 1/7/2024
                    'create_time_lt': end_time
                }     # Request body cho POST request theo notebook pattern - chá»‰ date filters
                # FULL LOAD: Láº¥y orders tá»« 1 nÄƒm trÆ°á»›c Ä‘á»ƒ trÃ¡nh timeout
                request_body = {
                    'create_time_ge': int(datetime(2023, 1, 1).timestamp()),  # Tá»« 2023 thay vÃ¬ 2020
                    'create_time_lt': end_time
                }ebook Ä‘Ã£ hoáº¡t Ä‘á»™ng (Steps 6-8)
"""

import requests
import time
import json
import pandas as pd
import logging
import sys
import os
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta

# Add project root to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))
from src.utils.auth import TikTokAuthenticator
from src.utils.logging import setup_logging
from config.settings import settings

logger = setup_logging("tiktok_shop_extractor")


class TikTokShopOrderExtractor:
    """TrÃ­ch xuáº¥t dá»¯ liá»‡u Ä‘Æ¡n hÃ ng tá»« TikTok Shop API"""

    def __init__(self):
        self.auth = TikTokAuthenticator()
        self.base_url = "https://open-api.tiktokglobalshop.com"
        self.logger = logger  # Add logger reference

    def stream_orders_lightweight(
        self,
        start_time: int,
        end_time: int,
        batch_size: int = 20,  # Increased for full load
        order_status: Optional[str] = None,
    ):
        """
        Generator Ä‘á»ƒ stream orders theo batch nhá» - trÃ¡nh OOM hoÃ n toÃ n
        Yield tá»«ng batch orders thay vÃ¬ load táº¥t cáº£ vÃ o memory
        """
        try:
            if not self.auth.ensure_valid_token():
                raise RuntimeError("Cannot authenticate with TikTok Shop API")

            cursor = ""
            page_number = 1
            total_processed = 0

            self.logger.info(
                f"Starting lightweight streaming with batch_size={batch_size}"
            )

            while True:
                self.logger.info(f"Streaming page {page_number}, cursor: '{cursor}'")

                # Parameters cho API call
                params = {
                    "app_key": self.auth.app_key,
                    "timestamp": str(int(time.time())),
                    "shop_cipher": self.auth.shop_cipher,
                    "sign_method": "hmac_sha256",
                    "page_size": batch_size,  # Increased for full load efficiency
                    "sort_order": "DESC",
                    "sort_field": "create_time",
                }

                if cursor:
                    params["cursor"] = cursor

                # Request body
                request_body = {
                    "create_time_ge": start_time,
                    "create_time_lt": end_time,
                }

                body_string = json.dumps(
                    request_body, separators=(",", ":"), sort_keys=True
                )
                signature = self.auth.generate_signature(
                    "/order/202309/orders/search", params, body_string
                )
                params["sign"] = signature

                headers = {
                    "x-tts-access-token": self.auth.access_token,
                    "Content-Type": "application/json",
                }

                url = f"{self.auth.base_api_url}/order/202309/orders/search"

                # API call
                response = requests.post(
                    url, params=params, data=body_string, headers=headers, timeout=30
                )

                if response.status_code == 200:
                    data = response.json()
                    if data.get("code") == 0:
                        orders = data.get("data", {}).get("orders", [])

                        if orders:
                            total_processed += len(orders)
                            self.logger.info(
                                f"Page {page_number}: Yielding {len(orders)} orders, total processed: {total_processed}"
                            )

                            # YIELD batch nhá» Ä‘á»ƒ khÃ´ng tÃ­ch tá»¥ trong memory
                            yield orders

                        # Check pagination
                        data_section = data.get("data", {})
                        more = data_section.get("more", False)
                        next_cursor = data_section.get("next_cursor", "")

                        self.logger.info(
                            f"Pagination info - more: {more}, next_cursor: '{next_cursor}'"
                        )

                        if more and next_cursor:
                            cursor = next_cursor
                            page_number += 1
                            self.logger.info(
                                f"Continuing to next page with cursor: {cursor}"
                            )
                            time.sleep(0.5)  # Rate limiting
                        else:
                            # Stop pagination if more=False or no next_cursor
                            self.logger.info(
                                f"Streaming complete. Total processed: {total_processed}"
                            )
                            break

                    else:
                        error_message = f"API Error: {data.get('message')}"
                        self.logger.error(error_message)
                        raise RuntimeError(error_message)
                else:
                    error_message = (
                        f"HTTP Error {response.status_code}: {response.text}"
                    )
                    self.logger.error(error_message)
                    raise RuntimeError(error_message)

        except Exception as e:
            self.logger.error(f"Error in stream_orders_lightweight: {e}")
            raise RuntimeError("Streaming extraction failed")

    def search_orders_for_ids(
        self,
        start_time: int,
        end_time: int,
        order_status: Optional[str] = None,
        page_size: int = 100,
    ) -> List[str]:  # FIXED: TÄƒng tá»« 10 lÃªn 100 Ä‘á»ƒ láº¥y nhiá»u orders hÆ¡n
        """
        TÃ¬m kiáº¿m ID Ä‘Æ¡n hÃ ng - LIGHTWEIGHT vá»›i page_size nhá» Ä‘á»ƒ trÃ¡nh OOM
        """
        try:
            # Ensure valid authentication
            if not self.auth.ensure_valid_token():
                raise RuntimeError("Cannot authenticate with TikTok Shop API")

            all_order_ids = []
            page_token = ""  # TikTok Shop sá»­ dá»¥ng page_token
            page_number = 1

            # Chá»‰ log starting message náº¿u khÃ´ng á»Ÿ QUIET mode
            if self.logger.level <= logging.INFO:
                self.logger.info(
                    f"Starting pagination search with page_size={page_size}"
                )

            while True:
                # Chá»‰ log náº¿u khÃ´ng á»Ÿ QUIET mode (WARNING level)
                if self.logger.level <= logging.INFO:
                    self.logger.info(
                        f"Fetching page {page_number}, page_token: '{page_token}'"
                    )

                # Parameters theo TikTok Shop API specification
                params = {
                    "app_key": self.auth.app_key,
                    "timestamp": str(int(time.time())),
                    "shop_cipher": self.auth.shop_cipher,
                    "sign_method": "hmac_sha256",
                    "page_size": page_size,
                    "sort_order": "DESC",
                    "sort_field": "create_time",
                }

                if page_token:
                    params["page_token"] = page_token

                # Request body cho POST request theo TikTok Shop API specification
                request_body = {
                    "create_time_ge": start_time,
                    "create_time_lt": end_time,
                }

                # Serialize body string Ä‘á»ƒ dÃ¹ng cho signature vÃ  request
                body_string = json.dumps(
                    request_body, separators=(",", ":"), sort_keys=True
                )

                # Generate signature vá»›i body
                signature = self.auth.generate_signature(
                    "/order/202309/orders/search", params, body_string
                )
                params["sign"] = signature

                headers = {
                    "x-tts-access-token": self.auth.access_token,
                    "Content-Type": "application/json",
                }

                url = f"{self.auth.base_api_url}/order/202309/orders/search"

                # POST request vá»›i body theo notebook pattern
                response = requests.post(
                    url, params=params, data=body_string, headers=headers, timeout=30
                )

                if response.status_code == 200:
                    data = response.json()
                    if data.get("code") == 0:
                        orders = data.get("data", {}).get("orders", [])
                        order_ids = [order["id"] for order in orders]
                        all_order_ids.extend(order_ids)

                        # Chá»‰ log náº¿u khÃ´ng á»Ÿ QUIET mode
                        if self.logger.level <= logging.INFO:
                            self.logger.info(
                                f"Page {page_number}: Found {len(order_ids)} orders, total: {len(all_order_ids)}"
                            )

                        # Check pagination theo TikTok Shop API response format
                        data_section = data.get("data", {})
                        next_page_token = data_section.get("next_page_token", "")
                        total_count = data_section.get("total_count", 0)

                        # Log pagination status chá»‰ khi DEBUG level
                        if self.logger.level <= logging.DEBUG:
                            self.logger.debug(
                                f"Pagination info - next_page_token: '{next_page_token}', total: {total_count}"
                            )

                        # Check for more pages theo API spec
                        if not next_page_token:
                            if self.logger.level <= logging.DEBUG:
                                self.logger.debug("No more pages available")
                            break

                        # Continue to next page
                        page_token = next_page_token
                        page_number += 1

                        if self.logger.level <= logging.DEBUG:
                            self.logger.debug(f"Continuing to page {page_number}")
                        time.sleep(0.5)  # Rate limiting

                    else:
                        error_message = f"API Error: {data.get('message')}"
                        self.logger.error(error_message)
                        raise RuntimeError(error_message)
                else:
                    error_message = (
                        f"HTTP Error {response.status_code}: {response.text}"
                    )
                    self.logger.error(error_message)
                    raise RuntimeError(error_message)

            # Log final count summary (luÃ´n log Ä‘á»ƒ cÃ³ tá»•ng káº¿t)
            self.logger.info(f"Total order IDs found: {len(all_order_ids)}")
            return all_order_ids

        except Exception as e:
            self.logger.error(f"Error in search_orders_for_ids: {e}")
            raise RuntimeError(
                "KhÃ´ng thá»ƒ thá»±c hiá»‡n request - cÃ³ thá»ƒ do lá»—i token hoáº·c káº¿t ná»‘i database"
            )

    def get_order_details_with_ids(self, order_ids: List[str]) -> List[Dict[str, Any]]:
        """
        Get order details theo pattern tá»« notebook - sá»­ dá»¥ng GET request vá»›i order_id_list
        """
        try:
            if not order_ids:
                return []

            # Ensure valid authentication
            if not self.auth.ensure_valid_token():
                raise RuntimeError("Cannot authenticate with TikTok Shop API")

            all_orders = []
            batch_size = 50  # API limit cho order details

            self.logger.info(
                f"Processing {len(order_ids)} order IDs in batches of {batch_size}"
            )

            for i in range(0, len(order_ids), batch_size):
                batch_ids = order_ids[i : i + batch_size]
                batch_number = i // batch_size + 1

                self.logger.info(
                    f"Processing batch {batch_number}/{(len(order_ids)-1)//batch_size + 1}: {len(batch_ids)} orders"
                )

                params = {
                    "app_key": self.auth.app_key,
                    "timestamp": str(int(time.time())),
                    "shop_cipher": self.auth.shop_cipher,
                    "sign_method": "hmac_sha256",
                    "ids": ",".join(batch_ids),  # Process ALL batch_ids, no limit
                }

                # Generate signature
                signature = self.auth.generate_signature("/order/202309/orders", params)
                params["sign"] = signature

                headers = {
                    "x-tts-access-token": self.auth.access_token,
                    "Content-Type": "application/json",
                }

                url = f"{self.auth.base_api_url}/order/202309/orders"

                response = requests.get(url, params=params, headers=headers, timeout=30)

                if response.status_code == 200:
                    data = response.json()
                    if data.get("code") == 0:
                        # Debug: Log full response structure
                        self.logger.info(f"Response data keys: {list(data.keys())}")
                        if "data" in data:
                            self.logger.info(
                                f"Data section keys: {list(data['data'].keys())}"
                            )

                        # Try multiple possible keys for orders
                        orders = []
                        data_section = data.get("data", {})
                        if "order_list" in data_section:
                            orders = data_section.get("order_list", [])
                            self.logger.info(f"Found orders in 'order_list' field")
                        elif "orders" in data_section:
                            orders = data_section.get("orders", [])
                            self.logger.info(f"Found orders in 'orders' field")
                        else:
                            self.logger.warning(
                                f"No orders found. Available keys: {list(data_section.keys())}"
                            )

                        all_orders.extend(orders)
                        self.logger.info(
                            f"Retrieved {len(orders)} orders from batch {i//batch_size + 1}"
                        )
                    else:
                        error_message = f"API error: {data.get('message')}"
                        self.logger.error(error_message)
                        raise RuntimeError(error_message)
                else:
                    error_message = (
                        f"HTTP error {response.status_code}: {response.text}"
                    )
                    self.logger.error(error_message)
                    raise RuntimeError(error_message)

                # Rate limiting
                time.sleep(0.5)

            self.logger.info(f"Total orders retrieved: {len(all_orders)}")
            return all_orders

        except Exception as e:
            self.logger.error(f"Error in get_order_details_with_ids: {e}")
            raise RuntimeError(
                "KhÃ´ng thá»ƒ thá»±c hiá»‡n request - cÃ³ thá»ƒ do lá»—i token hoáº·c káº¿t ná»‘i database"
            )

    def extract_orders_for_period(
        self, start_date: datetime, end_date: datetime, batch_size: int = 20
    ) -> List[Dict[str, Any]]:
        """
        Extract all orders for a specific time period vá»›i memory optimization

        Args:
            start_date: Start date
            end_date: End date
            batch_size: Sá»‘ orders xá»­ lÃ½ má»—i batch Ä‘á»ƒ trÃ¡nh OOM

        Returns:
            List of order dictionaries
        """
        try:
            # Convert to timestamps
            start_timestamp = int(start_date.timestamp())
            end_timestamp = int(end_date.timestamp())

            self.logger.info(f"Extracting orders from {start_date} to {end_date}")

            # Step 1: Search for order IDs vá»›i pagination
            order_ids = self.search_orders_for_ids(start_timestamp, end_timestamp)

            if not order_ids:
                self.logger.warning("No order IDs found for the specified period")
                return []

            self.logger.info(
                f"Found {len(order_ids)} order IDs. Processing in batches of {batch_size}"
            )

            # Step 2: Process orders in batches Ä‘á»ƒ trÃ¡nh OOM
            all_orders = []
            total_batches = (len(order_ids) + batch_size - 1) // batch_size

            for i in range(0, len(order_ids), batch_size):
                batch_num = (i // batch_size) + 1
                batch_order_ids = order_ids[i : i + batch_size]

                self.logger.info(
                    f"Processing batch {batch_num}/{total_batches}: {len(batch_order_ids)} orders"
                )

                try:
                    # Get detailed order information cho batch nÃ y
                    batch_orders = self.get_order_details_with_ids(batch_order_ids)
                    all_orders.extend(batch_orders)

                    self.logger.info(
                        f"Batch {batch_num} completed. Total orders so far: {len(all_orders)}"
                    )

                    # Memory cleanup sau má»—i batch
                    del batch_orders
                    import gc

                    gc.collect()

                    # Rate limiting giá»¯a cÃ¡c batches
                    if batch_num < total_batches:
                        time.sleep(1)  # 1 second delay giá»¯a batches

                except Exception as e:
                    self.logger.error(f"Error processing batch {batch_num}: {str(e)}")
                    # Continue vá»›i batch tiáº¿p theo thay vÃ¬ fail toÃ n bá»™
                    continue

            self.logger.info(f"Successfully extracted {len(all_orders)} orders")
            return all_orders

        except Exception as e:
            self.logger.error(f"Exception in extract_orders_for_period: {str(e)}")
            raise

    def extract_recent_orders(self, days_back: int = 1) -> List[Dict[str, Any]]:
        """
        Extract orders from recent days

        Args:
            days_back: Number of days to look back

        Returns:
            List of order dictionaries
        """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        return self.extract_orders_for_period(start_date, end_date)

    def extract_incremental_orders(
        self, minutes_back: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Extract orders updated in the last N minutes for incremental ETL

        Args:
            minutes_back: Number of minutes to look back (default 10 for standardized incremental window)

        Returns:
            List of order dictionaries from recent updates
        """
        try:
            end_time = datetime.now()
            start_time = end_time - timedelta(minutes=minutes_back)

            self.logger.info(
                f"Extracting incremental orders from {start_time.strftime('%Y-%m-%d %H:%M')} to {end_time.strftime('%Y-%m-%d %H:%M')}"
            )

            # Use update_time instead of create_time for incremental
            start_timestamp = int(start_time.timestamp())
            end_timestamp = int(end_time.timestamp())

            # Get order IDs first
            order_ids = self.search_orders_for_ids(
                create_time_ge=start_timestamp,
                create_time_lt=end_timestamp,
                page_size=50,  # Smaller batch for 10-min incremental window
            )

            if not order_ids:
                self.logger.info("No new/updated orders found in incremental window")
                return []

            # Extract full details
            orders = self.get_order_details_batch(order_ids)

            self.logger.info(f"Extracted {len(orders)} orders for incremental update")
            return orders

        except Exception as e:
            self.logger.error(f"Error in extract_incremental_orders: {str(e)}")
            raise

    def test_api_connection(self) -> bool:
        """
        Kiá»ƒm tra káº¿t ná»‘i API vÃ  xÃ¡c thá»±c

        Returns:
            True náº¿u káº¿t ná»‘i thÃ nh cÃ´ng, False náº¿u ngÆ°á»£c láº¡i
        """
        try:
            logger.info("Äang kiá»ƒm tra káº¿t ná»‘i TikTok API...")

            # Kiá»ƒm tra xÃ¡c thá»±c vÃ  láº¥y shop cipher
            if self.auth.ensure_valid_token():
                logger.info("âœ“ XÃ¡c thá»±c thÃ nh cÃ´ng")
                logger.info(f"âœ“ Shop cipher: {self.auth.shop_cipher}")

                # Kiá»ƒm tra má»™t API call Ä‘Æ¡n giáº£n
                test_start = datetime.now() - timedelta(days=1)
                test_end = datetime.now()

                order_ids = self.search_orders_for_ids(
                    int(test_start.timestamp()), int(test_end.timestamp()), page_size=1
                )

                logger.info(
                    f"âœ“ API call thÃ nh cÃ´ng, tÃ¬m tháº¥y {len(order_ids)} Ä‘Æ¡n hÃ ng trong 24h qua"
                )
                return True
            else:
                logger.error("âœ— XÃ¡c thá»±c tháº¥t báº¡i")
                return False

        except Exception as e:
            logger.error(f"âœ— Kiá»ƒm tra káº¿t ná»‘i API tháº¥t báº¡i: {str(e)}")
            return False

    def find_earliest_order_date(
        self, max_lookback_years: int = 2
    ) -> Optional[datetime]:
        """
        Tá»± Ä‘á»™ng tÃ¬m ngÃ y Ä‘Æ¡n hÃ ng Ä‘áº§u tiÃªn trong há»‡ thá»‘ng

        Args:
            max_lookback_years: Sá»‘ nÄƒm tá»‘i Ä‘a Ä‘á»ƒ tÃ¬m kiáº¿m ngÆ°á»£c (máº·c Ä‘á»‹nh 2 nÄƒm)

        Returns:
            Datetime cá»§a Ä‘Æ¡n hÃ ng Ä‘áº§u tiÃªn hoáº·c None náº¿u khÃ´ng tÃ¬m tháº¥y
        """
        try:
            logger.info(
                f"Searching earliest order in last {max_lookback_years} years..."
            )

            # Báº¯t Ä‘áº§u tá»« hiá»‡n táº¡i vÃ  tÃ¬m ngÆ°á»£c
            end_date = datetime.now()
            earliest_found = None

            # Test cÃ¡c khoáº£ng thá»i gian khÃ¡c nhau (giá»›i háº¡n 2 nÄƒm = 730 ngÃ y)
            test_periods = [30, 90, 180, 365, 730]  # days

            for days_back in test_periods:
                start_date = end_date - timedelta(days=days_back)
                start_timestamp = int(start_date.timestamp())
                end_timestamp = int(end_date.timestamp())

                # Log gá»n hÆ¡n
                logger.info(
                    f"Testing: {start_date.strftime('%Y-%m-%d')} ({days_back} days back)"
                )

                try:
                    # TÃ¬m 1 order trong khoáº£ng nÃ y vá»›i timeout ngáº¯n
                    order_ids = self.search_orders_for_ids(
                        start_timestamp, end_timestamp, page_size=1
                    )

                    if order_ids:
                        logger.info(
                            f"âœ“ Found data from {start_date.strftime('%Y-%m-%d')}"
                        )
                        earliest_found = start_date
                        # Tiáº¿p tá»¥c tÃ¬m Ä‘á»ƒ kiá»ƒm tra period dÃ i hÆ¡n
                    else:
                        logger.info(f"âœ— No data from {start_date.strftime('%Y-%m-%d')}")
                        break

                except Exception as e:
                    logger.warning(f"Error checking {days_back} days: {str(e)}")
                    # Náº¿u lá»—i quÃ¡ nhiá»u láº§n, dá»«ng láº¡i vÃ  dÃ¹ng fallback
                    if days_back <= 90:  # Náº¿u lá»—i ngay tá»« 90 ngÃ y trá»Ÿ láº¡i
                        logger.warning("Too many errors, using fallback date")
                        break
                    continue

            if earliest_found:
                logger.info(
                    f"ğŸ¯ Earliest date found: {earliest_found.strftime('%Y-%m-%d')}"
                )
                return earliest_found
            else:
                logger.warning("No orders found in search range")
                return None

        except Exception as e:
            logger.error(f"Error finding earliest order date: {str(e)}")
            return None

    def stream_orders_lightweight(
        self, start_time: int, end_time: int, batch_size: int = 20
    ) -> Any:
        """
        Stream orders trong batches nhá» Ä‘á»ƒ ETL real-time
        TrÃ¡nh load háº¿t memory vÃ  infinite loop

        Args:
            start_time: Unix timestamp báº¯t Ä‘áº§u
            end_time: Unix timestamp káº¿t thÃºc
            batch_size: Sá»‘ orders per batch

        Yields:
            Batches of order details
        """
        try:
            self.logger.info(
                f"ğŸš€ Starting lightweight streaming from {start_time} to {end_time}"
            )
            self.logger.info(f"ğŸ“¦ Batch size: {batch_size}")

            # Phase 1: Get all order IDs first
            self.logger.info("ğŸ“‹ Phase 1: Getting all order IDs...")
            order_ids = self.search_orders_for_ids(
                start_time, end_time, page_size=100
            )  # FIXED: TÄƒng tá»« 50 lÃªn 100

            if not order_ids:
                self.logger.warning("âš ï¸ No orders found in the specified time range")
                return

            total_orders = len(order_ids)
            self.logger.info(f"ğŸ“Š Found {total_orders} orders total")

            # Phase 2: Stream order details in small batches
            self.logger.info("ğŸ“¦ Phase 2: Streaming order details...")
            for i in range(0, total_orders, batch_size):
                batch_ids = order_ids[i : i + batch_size]
                batch_number = (i // batch_size) + 1
                total_batches = (total_orders + batch_size - 1) // batch_size

                self.logger.info(
                    f"ğŸ”„ Processing batch {batch_number}/{total_batches} ({len(batch_ids)} orders)"
                )

                try:
                    # Get detailed order data for this batch
                    batch_orders = self.get_order_details_with_ids(batch_ids)

                    if batch_orders:
                        self.logger.info(
                            f"âœ… Successfully extracted {len(batch_orders)} orders in batch {batch_number}"
                        )
                        yield batch_orders
                    else:
                        self.logger.warning(
                            f"âš ï¸ No orders returned for batch {batch_number}"
                        )

                except Exception as batch_error:
                    self.logger.error(
                        f"âŒ Error processing batch {batch_number}: {str(batch_error)}"
                    )
                    # Continue with next batch instead of failing entire stream
                    continue

                # Small delay between batches Ä‘á»ƒ trÃ¡nh rate limiting
                time.sleep(0.3)

            self.logger.info(
                f"âœ… Streaming completed - processed {total_orders} orders"
            )

        except Exception as e:
            self.logger.error(f"âŒ Critical error in streaming: {str(e)}")
            raise

            self.logger.info(f"ğŸš€ Báº¯t Ä‘áº§u full historical extraction:")
            self.logger.info(
                f"   ğŸ“… Tá»«: {datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')}"
            )
            self.logger.info(
                f"   ğŸ“… Äáº¿n: {datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S')}"
            )
            self.logger.info(f"   ğŸ“¦ Batch size: {batch_size}")

            # Stream orders tá»« start_time Ä‘áº¿n end_time
            yield from self.stream_orders_lightweight(
                start_time=start_time, end_time=end_time, batch_size=batch_size
            )

        except Exception as e:
            self.logger.error(f"Error in stream_all_historical_orders: {str(e)}")
            raise
